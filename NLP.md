# Natural Language Processing

## Paper List

|Paper title|Year|Published at|Category|URLs|
|-----------|----|------------|--------|----|
|Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection|2011|NIPS|Neural Models|[PDF](https://papers.nips.cc/paper/4204-dynamic-pooling-and-unfolding-recursive-autoencoders-for-paraphrase-detection.pdf)|
|"Why Should I Trust You?": Explaining the Predictions of Any Classifier|2016|KDD|ML|[PDF](https://arxiv.org/pdf/1602.04938)|
|Beyond Accuracy: Behavioral Testing of NLP Models with CheckList|2020|ACL|ML|[PDF](https://www.aclweb.org/anthology/2020.acl-main.442.pdf)|
|Natural Language Processing (almost) from Scratch|2011|JMLR|Survey|[PDF](https://arxiv.org/pdf/1103.0398.pdf)|
|Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank|2013|EMNLP||[PDF](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)|
|Character-level Convolutional Networks for Text Classification|2015|NIPS||[PDF](https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf)|
|Convolutional Neural Networks for Sentence Classification|2014|EMNLP||[PDF](https://arxiv.org/pdf/1408.5882)|
|Deep contextualized word representations|2018|ACL||[PDF](https://arxiv.org/pdf/1802.05365)|
|BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding|2019|ACL||[PDF](https://arxiv.org/pdf/1810.04805)|
|RoBERTa: A Robustly Optimized BERT Pretraining Approach|2020|||[PDF](https://arxiv.org/pdf/1907.11692)|
|Efficient Estimation of Word Representations in Vector Space|2013|ICLR (workshop)||[PDF](https://arxiv.org/pdf/1301.3781)|
|Distributed Representations of Words and Phrases and their Compositionality|2013|NIPS||[PDF](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)|
|Distributed Representations of Sentences and Documents|2014|ICML||[PDF](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)|
|GloVe: Global Vectors for Word Representation|2014|EMNLP||[PDF](https://nlp.stanford.edu/pubs/glove.pdf)|
|Skip-Thought Vectors|2015|NIPS||[PDF](https://papers.nips.cc/paper/5950-skip-thought-vectors.pdf)|
|Enriching Word Vectors with Subword Information|2017|ACL||[PDF](https://www.aclweb.org/anthology/Q17-1010.pdf)|
|Universal Sentence Encoder for English|2018|EMNLP||[PDF](https://arxiv.org/pdf/1803.11175)|
|The Unreasonable Effectiveness of Recurrent Neural Networks|2015|Blog post||[Blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)|
|Character-Aware Neural Language Models|2016|AAAI||[PDF](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewFile/12489/12017)|
|Language Models are Unsupervised Multitask Learners|2018|||[PDF](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)|
|Globally Normalized Transition-Based Neural Networks|2016||POS Tagging|[PDF](https://research.google.com/pubs/archive/45377.pdf)|
|Grammar as a Foreign Language|2015|NIPS||[PDF](https://papers.nips.cc/paper/5635-grammar-as-a-foreign-language.pdf)|
|Sequence to Sequence Learning with Neural Networks|2014|NIPS||[PDF](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)|
|Effective Approaches to Attention-based Neural Machine Translation|2015|EMNLP||[PDF](https://www-nlp.stanford.edu/pubs/emnlp15_attn.pdf)|
|Neural Machine Translation of Rare Words with Subword Units|2016|ACL||[PDF](https://arxiv.org/pdf/1508.07909)|
|Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation|2016|||[PDF](https://arxiv.org/pdf/1609.08144)|
|Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation|2017|ACL||[PDF](https://arxiv.org/pdf/1611.04558)|
|Convolutional Sequence to Sequence Learning|2017|ICML||[PDF](https://arxiv.org/pdf/1705.03122)|
|Attention Is All You Need|2017|NIPS||[PDF](https://arxiv.org/pdf/1706.03762)|
|A Neural Attention Model for Sentence Summarization|2015|EMNLP||[PDF](https://aclweb.org/anthology/D/D15/D15-1044.pdf)|
|Get To The Point: Summarization with Pointer-Generator Networks|2017|ACL||[PDF](https://nlp.stanford.edu/pubs/see2017get.pdf)|
|SQuAD: 100,000+ Questions for Machine Comprehension of Text|2016|EMNLP||[PDF](https://arxiv.org/pdf/1606.05250)|
|Bi-Directional Attention Flow for Machine Comprehension|2015|ICLR||[PDF](https://arxiv.org/pdf/1611.01603)|
|Deep Reinforcement Learning for Dialogue Generation|2016|EMNLP||[PDF](https://arxiv.org/pdf/1606.01541)|
|Sequence Level Training with Recurrent Neural Networks|2016|ICLR||[PDF](https://arxiv.org/pdf/1511.06732)|
|Generating sentences from a continuous space|2016|CoNLL||[PDF](http://www.aclweb.org/anthology/K16-1002.pdf)|
|SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient|2017|AAAI||[PDF](https://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14344/14489)|
